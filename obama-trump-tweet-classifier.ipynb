{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TWEETS_DIR = './obama-trump-data/tweets'\n",
    "LABELS_DIR = './obama-trump-data/labels_np'\n",
    "\n",
    "EMBEDDINGS_DIR = \"./glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:  \"America should be very proud.\" President Obama #LoveWins . Label:  0\n",
      "Tweet:  TO ALL AMERICANS\n",
      "https://t.co/D7Es6ie4fY . Label:  1\n",
      "Tweet:  No deal was made last night on DACA. Massive border security would have to be agreed to in exchange for consent. Would be subject to vote. . Label:  1\n",
      "Tweet:  I was viciously attacked by Mr. Khan at the Democratic Convention. Am I not allowed to respond? Hillary voted for the Iraq war, not me! . Label:  1\n",
      "Tweet:  .@RudyGiuliani, one of the finest people I know and a former GREAT Mayor of N.Y.C., just took himself out of consideration for \"State\". . Label:  1\n",
      "Tweet:  The #TPP establishes the highest labor standards of any trade agreement in history. http://t.co/3vqoancyYp . Label:  0\n",
      "Tweet:  Thoughts and prayers to the great people of Indiana. You will prevail! . Label:  1\n",
      "Tweet:  Hillary said she was under sniper fire (while surrounded by USSS.) Turned out to be a total lie. She is not fit to https://t.co/hBIrGj21l6 . Label:  1\n",
      "Tweet:  The failing @nytimes has become a newspaper of fiction. Their stories about me always quote non-existent unnamed sources. Very dishonest! . Label:  1\n",
      "Tweet:  Thank you Council Bluffs, Iowa! Will be back soon. Remember- everything you need to know about Hillary -- just https://t.co/45kIHxdX83 . Label:  1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load tweets, labels\n",
    "tweets = pickle.load(open(TWEETS_DIR,'rb'))\n",
    "labels = pickle.load(open(LABELS_DIR,'rb'))\n",
    "\n",
    "# Sample some tweets to display\n",
    "for i in range(0,100,10):\n",
    "    print (\"Tweet: \", tweets[i], \". Label: \", labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13049 unique tokens (words).\n",
      "Training data size is (6136,31)\n",
      "Labels are size 6136\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the tweets (convert sentence to sequence of words)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens (words).' % len(word_index))\n",
    "\n",
    "# Pad sequences to ensure samples are the same size\n",
    "training_data = pad_sequences(sequences)\n",
    "\n",
    "print(\"Training data size is (%d,%d)\"  % training_data.shape) # shape = (num. tweets, max. length of tweet)\n",
    "print(\"Labels are size %d\"  % labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:  \"America should be very proud.\" President Obama #LoveWins\n",
      "Tweet after tokenization and padding:  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  44  88  21  83 391  12  14 794]\n"
     ]
    }
   ],
   "source": [
    "# Show effect of tokenization, padding\n",
    "print (\"Original tweet: \", tweets[0])\n",
    "print (\"Tweet after tokenization and padding: \", training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Convert words to word embedding vectors\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDINGS_DIR)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# prepare word embedding matrix\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2645\n",
      "[ 0.079084   -0.81503999  1.79009998  0.91653001  0.10797    -0.55628002\n",
      " -0.84426999 -1.49510002  0.13417999  0.63626999  0.35146001  0.25813001\n",
      " -0.55028999  0.51055998  0.37408999  0.12092    -1.61660004  0.83653003\n",
      "  0.14202    -0.52348     0.73452997  0.12207    -0.49079001  0.32532999\n",
      "  0.45306    -1.58500004 -0.63848001 -1.00530005  0.10454    -0.42984\n",
      "  3.18099999 -0.62186998  0.16819    -1.01390004  0.064058    0.57844001\n",
      " -0.45559999  0.73782998  0.37202999 -0.57722002  0.66441     0.055129\n",
      "  0.037891    1.32749999  0.30991     0.50696999  1.23570001  0.1274\n",
      " -0.11434     0.20709001]\n"
     ]
    }
   ],
   "source": [
    "# Sample word embedding vector:\n",
    "print(word_index[\"computer\"]) # retrieve word index\n",
    "print(embedding_matrix[2645]) # use word index to retrieve word embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers import Dropout, concatenate\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras import metrics\n",
    "from keras.models import Model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 31, 50)            652500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 672,751\n",
      "Trainable params: 20,251\n",
      "Non-trainable params: 652,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Add pre-trained embedding layer \n",
    "# converts word indices to GloVe word embedding vectors as they're fed in\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=training_data.shape[1],\n",
    "                    trainable=False))\n",
    "\n",
    "model.add(LSTM(units = 50, activation = 'tanh', recurrent_activation = 'hard_sigmoid', return_sequences = False))\n",
    "model.add(Dense(units = 1))\n",
    "model.add(Activation('sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "LOSS = 'binary_crossentropy'\n",
    "OPTIMIZER = 'RMSprop'\n",
    "\n",
    "model.compile(loss = LOSS, optimizer = OPTIMIZER, metrics = [metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4908 samples, validate on 1228 samples\n",
      "Epoch 1/8\n",
      "4908/4908 [==============================] - 46s - loss: 0.3464 - binary_accuracy: 0.8490 - val_loss: 0.2640 - val_binary_accuracy: 0.8860\n",
      "Epoch 2/8\n",
      "4908/4908 [==============================] - 48s - loss: 0.2158 - binary_accuracy: 0.9142 - val_loss: 0.2491 - val_binary_accuracy: 0.8925\n",
      "Epoch 3/8\n",
      "4908/4908 [==============================] - 49s - loss: 0.1760 - binary_accuracy: 0.9328 - val_loss: 0.2403 - val_binary_accuracy: 0.9047\n",
      "Epoch 4/8\n",
      "4908/4908 [==============================] - 46s - loss: 0.1532 - binary_accuracy: 0.9393 - val_loss: 0.1593 - val_binary_accuracy: 0.9332\n",
      "Epoch 5/8\n",
      "4908/4908 [==============================] - 46s - loss: 0.1321 - binary_accuracy: 0.9505 - val_loss: 0.1534 - val_binary_accuracy: 0.9373\n",
      "Epoch 6/8\n",
      "4908/4908 [==============================] - 46s - loss: 0.1198 - binary_accuracy: 0.9540 - val_loss: 0.1538 - val_binary_accuracy: 0.9357\n",
      "Epoch 7/8\n",
      "4908/4908 [==============================] - 46s - loss: 0.1053 - binary_accuracy: 0.9595 - val_loss: 0.1358 - val_binary_accuracy: 0.9503\n",
      "Epoch 8/8\n",
      "4908/4908 [==============================] - 46s - loss: 0.0917 - binary_accuracy: 0.9680 - val_loss: 0.2286 - val_binary_accuracy: 0.9267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11e0a72b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 8\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "model.fit(training_data, labels, \n",
    "          epochs = EPOCHS, \n",
    "          batch_size = BATCH_SIZE, \n",
    "          validation_split =.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:caispp]",
   "language": "python",
   "name": "conda-env-caispp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
